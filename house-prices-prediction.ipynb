{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30407,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# House Prices Prediction","metadata":{"id":"5v5mm4amQRrm","papermill":{"duration":0.010092,"end_time":"2023-03-07T06:21:39.774967","exception":false,"start_time":"2023-03-07T06:21:39.764875","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Week 7 — Data Understanding & Feature Engineering\n### Part 1: Exploratory Data Analysis & Baseline\n### Part 2: Feature Engineering & Updated Baseline\n","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nDATA_DIR = \"/kaggle/input/house-prices-advanced-regression-techniques\" \n\ntrain = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\ntest  = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n\nprint(\"train:\", train.shape, \"test:\", test.shape)\nprint(\"target exists:\", \"SalePrice\" in train.columns, \"SalePrice in test:\", \"SalePrice\" in test.columns)\nprint(\"head cols:\", train.columns[:10].tolist())\ntrain.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:53:26.469160Z","iopub.execute_input":"2026-02-05T00:53:26.469654Z","iopub.status.idle":"2026-02-05T00:53:26.551262Z","shell.execute_reply.started":"2026-02-05T00:53:26.469597Z","shell.execute_reply":"2026-02-05T00:53:26.549672Z"}},"outputs":[{"name":"stdout","text":"train: (1460, 81) test: (1459, 80)\ntarget exists: True SalePrice in test: False\nhead cols: ['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities']\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n\n  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n\n  YrSold  SaleType  SaleCondition  SalePrice  \n0   2008        WD         Normal     208500  \n1   2007        WD         Normal     181500  \n2   2008        WD         Normal     223500  \n3   2006        WD        Abnorml     140000  \n4   2008        WD         Normal     250000  \n\n[5 rows x 81 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 81 columns</p>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"import numpy as np\nimport os\n\nOUT_EDA = \"outputs/eda\"\nos.makedirs(OUT_EDA, exist_ok=True)\n\ntarget = \"SalePrice\"\ny = train[target].copy()\nX = train.drop(columns=[target])\n\n# 1) missing rate\nmissing = (train.isna().mean()\n           .sort_values(ascending=False)\n           .reset_index())\nmissing.columns = [\"feature\", \"missing_rate\"]\nmissing.to_csv(os.path.join(OUT_EDA, \"missing_rate.csv\"), index=False)\n\n# 2) feature type summary\nnum_cols = X.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = [c for c in X.columns if c not in num_cols]\npd.DataFrame({\n    \"numeric_features\": [len(num_cols)],\n    \"categorical_features\": [len(cat_cols)],\n    \"total_features\": [X.shape[1]]\n}).to_csv(os.path.join(OUT_EDA, \"feature_type_summary.csv\"), index=False)\n\n# 3) target summary\npd.DataFrame({\n    \"SalePrice_mean\":[y.mean()],\n    \"SalePrice_std\":[y.std()],\n    \"SalePrice_skew\":[y.skew()],\n    \"SalePrice_kurt\":[y.kurt()]\n}).to_csv(os.path.join(OUT_EDA, \"target_summary.csv\"), index=False)\n\n# 4) top numeric correlations\ncorr = train[num_cols + [target]].corr()[target].sort_values(ascending=False)\ncorr.reset_index().rename(columns={\"index\":\"feature\", target:\"corr_with_target\"}) \\\n    .to_csv(os.path.join(OUT_EDA, \"top_numeric_correlations.csv\"), index=False)\n\nprint(\"Saved EDA files to:\", OUT_EDA)\nprint(os.listdir(OUT_EDA)[:20])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:53:26.664662Z","iopub.execute_input":"2026-02-05T00:53:26.665995Z","iopub.status.idle":"2026-02-05T00:53:26.719882Z","shell.execute_reply.started":"2026-02-05T00:53:26.665925Z","shell.execute_reply":"2026-02-05T00:53:26.718139Z"}},"outputs":[{"name":"stdout","text":"Saved EDA files to: outputs/eda\n['feature_type_summary.csv', 'missing_rate.csv', 'top_numeric_correlations.csv', 'target_summary.csv']\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\n# outputs direction\nOUT_MODELS = \"outputs/models\"\nos.makedirs(OUT_MODELS, exist_ok=True)\n\n# target / features\ntarget = \"SalePrice\"\ny = np.log1p(train[target])        # metric \nX = train.drop(columns=[target])\n\n# feature split\nnum_cols = X.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = [c for c in X.columns if c not in num_cols]\n\n# preprocess\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n])\n\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, num_cols),\n        (\"cat\", categorical_transformer, cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\n# model\nmodel = Ridge(alpha=10.0, random_state=42)\n\npipe = Pipeline(steps=[\n    (\"preprocess\", preprocess),\n    (\"model\", model),\n])\n\n# RMSE on log-space\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\nrmse_scorer = make_scorer(rmse, greater_is_better=False)\n\n# CV\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(pipe, X, y, cv=cv, scoring=rmse_scorer)\n\nresult = {\n    \"model\": \"Ridge(alpha=10.0)\",\n    \"target_transform\": \"log1p(SalePrice)\",\n    \"cv\": \"KFold(n_splits=5, shuffle=True, random_state=42)\",\n    \"metric\": \"RMSE on log-space (negative scorer)\",\n    \"rmse_mean\": float((-scores).mean()),\n    \"rmse_std\": float((-scores).std()),\n    \"rmse_each_fold\": [float(s) for s in (-scores)]\n}\n\nwith open(os.path.join(OUT_MODELS, \"baseline_ridge_cv.json\"), \"w\") as f:\n    json.dump(result, f, indent=2)\n\nprint(\"Baseline CV RMSE (log-space):\", result[\"rmse_mean\"], \"+/-\", result[\"rmse_std\"])\nprint(\"Saved:\", os.path.join(OUT_MODELS, \"baseline_ridge_cv.json\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:53:26.730936Z","iopub.execute_input":"2026-02-05T00:53:26.732782Z","iopub.status.idle":"2026-02-05T00:53:27.150103Z","shell.execute_reply.started":"2026-02-05T00:53:26.732694Z","shell.execute_reply":"2026-02-05T00:53:27.148340Z"}},"outputs":[{"name":"stdout","text":"Baseline CV RMSE (log-space): 0.19657044672518134 +/- 0.045085585064443365\nSaved: outputs/models/baseline_ridge_cv.json\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### Interpretation: Data Characteristics and Baseline Design\n\n\nExploratory analysis showed that the target variable (SalePrice) is right-skewed,which motivated a log-scale transformation to align with the evaluation metric andreduce heteroscedasticity. The dataset contains a large number of categorical featuresand non-random missing values, indicating that careful preprocessing and regularizationwould be necessary for building stable linear models.Based on these characteristics, a regularized linear regression model was selectedas a baseline to establish a reproducible performance reference before introducingadditional feature transformations.\n\n","metadata":{}},{"cell_type":"code","source":"# Feature Engineering (Week A - Part 2)\n\nX_fe = train.drop(columns=[\"SalePrice\"]).copy()\n\n# 1) Total square footage\nX_fe[\"TotalSF\"] = (\n    X_fe[\"TotalBsmtSF\"].fillna(0)\n    + X_fe[\"1stFlrSF\"]\n    + X_fe[\"2ndFlrSF\"]\n)\n\n# 2) House age\nX_fe[\"HouseAge\"] = X_fe[\"YrSold\"] - X_fe[\"YearBuilt\"]\n\n# 3) Is remodeled\nX_fe[\"IsRemodeled\"] = (X_fe[\"YearRemodAdd\"] != X_fe[\"YearBuilt\"]).astype(int)\n\n# 4) Total bathrooms\nX_fe[\"TotalBath\"] = (\n    X_fe[\"FullBath\"]\n    + 0.5 * X_fe[\"HalfBath\"]\n    + X_fe[\"BsmtFullBath\"]\n    + 0.5 * X_fe[\"BsmtHalfBath\"]\n)\n\nprint(X_fe[[\"TotalSF\", \"HouseAge\", \"IsRemodeled\", \"TotalBath\"]].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:53:27.153164Z","iopub.execute_input":"2026-02-05T00:53:27.153702Z","iopub.status.idle":"2026-02-05T00:53:27.176251Z","shell.execute_reply.started":"2026-02-05T00:53:27.153623Z","shell.execute_reply":"2026-02-05T00:53:27.174856Z"}},"outputs":[{"name":"stdout","text":"   TotalSF  HouseAge  IsRemodeled  TotalBath\n0     2566         5            0        3.5\n1     2524        31            0        2.5\n2     2706         7            1        3.5\n3     2473        91            1        2.0\n4     3343         8            0        3.5\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score\n\ny = np.log1p(train[\"SalePrice\"])\n\nnum_cols = X_fe.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = [c for c in X_fe.columns if c not in num_cols]\n\npreprocess_fe = ColumnTransformer(\n    transformers=[\n        (\"num\", Pipeline([\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n        ]), num_cols),\n        (\"cat\", Pipeline([\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n        ]), cat_cols),\n    ]\n)\n\npipe_fe = Pipeline(steps=[\n    (\"preprocess\", preprocess_fe),\n    (\"model\", Ridge(alpha=10.0, random_state=42)),\n])\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nscores_fe = cross_val_score(pipe_fe, X_fe, y, cv=cv, scoring=rmse_scorer)\n\nprint(\"FE Ridge CV RMSE (log):\",\n      (-scores_fe).mean(), \"+/-\", (-scores_fe).std())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:53:27.177747Z","iopub.execute_input":"2026-02-05T00:53:27.178768Z","iopub.status.idle":"2026-02-05T00:53:27.576327Z","shell.execute_reply.started":"2026-02-05T00:53:27.178724Z","shell.execute_reply":"2026-02-05T00:53:27.574828Z"}},"outputs":[{"name":"stdout","text":"FE Ridge CV RMSE (log): 0.19477724320511808 +/- 0.04547290816459814\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Interpretation: Feature Engineering Results\n\nThe engineered features resulted in a small but consistent improvement in\ncross-validated performance, while the variance across folds remained similar.\nThis suggests that the added features introduced useful signal without increasing\nmodel complexity or overfitting.\n\nThe magnitude of improvement is modest, which is expected for linear models applied\nto high-dimensional tabular data. More complex interactions and nonlinear effects\nare intentionally deferred to later modeling stages.\n","metadata":{}},{"cell_type":"code","source":"import os, json\n\nOUT_MODELS = \"outputs/models\"\nos.makedirs(OUT_MODELS, exist_ok=True)\n\nfe_result = {\n    \"model\": \"Ridge(alpha=10.0)\",\n    \"features\": \"baseline + engineered features (TotalSF, HouseAge, IsRemodeled, TotalBath)\",\n    \"target_transform\": \"log1p(SalePrice)\",\n    \"cv\": \"KFold(n_splits=5, shuffle=True, random_state=42)\",\n    \"metric\": \"RMSE on log-space\",\n    \"rmse_mean\": float((-scores_fe).mean()),\n    \"rmse_std\": float((-scores_fe).std()),\n    \"rmse_each_fold\": [float(s) for s in (-scores_fe)]\n}\n\nwith open(os.path.join(OUT_MODELS, \"fe_ridge_cv.json\"), \"w\") as f:\n    json.dump(fe_result, f, indent=2)\n\nprint(\"Saved:\", os.path.join(OUT_MODELS, \"fe_ridge_cv.json\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:53:27.577614Z","iopub.execute_input":"2026-02-05T00:53:27.578030Z","iopub.status.idle":"2026-02-05T00:53:27.589389Z","shell.execute_reply.started":"2026-02-05T00:53:27.577994Z","shell.execute_reply":"2026-02-05T00:53:27.587835Z"}},"outputs":[{"name":"stdout","text":"Saved: outputs/models/fe_ridge_cv.json\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### Design Considerations\n\nAlternative feature strategies such as neighborhood-level aggregations,\ninteraction terms, and ordinal encodings were considered but intentionally\ndeferred to isolate the effect of simple, interpretable feature transformations\nbefore introducing additional complexity.\n","metadata":{}},{"cell_type":"markdown","source":"## Week 8 — Regularization & Model Selection","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\n\n# target\ny = np.log1p(train[\"SalePrice\"])\n\n# feature split (FE fixed)\nnum_cols = X_fe.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = [c for c in X_fe.columns if c not in num_cols]\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))]), num_cols),\n        (\"cat\", Pipeline([\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n        ]), cat_cols),\n    ]\n)\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:53:27.593059Z","iopub.execute_input":"2026-02-05T00:53:27.593701Z","iopub.status.idle":"2026-02-05T00:53:27.613579Z","shell.execute_reply.started":"2026-02-05T00:53:27.593621Z","shell.execute_reply":"2026-02-05T00:53:27.611768Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import pandas as pd\n\nalphas = [0.1, 1.0, 10.0, 50.0, 100.0]\n\nresults = []\n\nfor model_name, model_cls, kwargs in [\n    (\"Ridge\", Ridge, {}),\n    (\"Lasso\", Lasso, {\"max_iter\": 5000}),\n    (\"ElasticNet\", ElasticNet, {\"l1_ratio\": 0.5, \"max_iter\": 5000}),\n]:\n    for a in alphas:\n        pipe = Pipeline(steps=[\n            (\"preprocess\", preprocess),\n            (\"model\", model_cls(alpha=a, random_state=42, **kwargs)),\n        ])\n        scores = cross_val_score(pipe, X_fe, y, cv=cv, scoring=rmse_scorer)\n        results.append({\n            \"model\": model_name,\n            \"alpha\": a,\n            \"rmse_mean\": float((-scores).mean()),\n            \"rmse_std\": float((-scores).std()),\n        })\n\ndf_results = pd.DataFrame(results).sort_values([\"rmse_mean\", \"rmse_std\"])\ndf_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:53:27.615392Z","iopub.execute_input":"2026-02-05T00:53:27.615834Z","iopub.status.idle":"2026-02-05T00:53:49.905479Z","shell.execute_reply.started":"2026-02-05T00:53:27.615792Z","shell.execute_reply":"2026-02-05T00:53:49.903837Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"         model  alpha  rmse_mean  rmse_std\n10  ElasticNet    0.1   0.179957  0.049057\n5        Lasso    0.1   0.189868  0.047422\n11  ElasticNet    1.0   0.191479  0.050963\n3        Ridge   50.0   0.194772  0.045467\n4        Ridge  100.0   0.194774  0.045474\n0        Ridge    0.1   0.194775  0.045473\n2        Ridge   10.0   0.194777  0.045473\n1        Ridge    1.0   0.194778  0.045473\n6        Lasso    1.0   0.198569  0.052737\n12  ElasticNet   10.0   0.233650  0.050588\n7        Lasso   10.0   0.238661  0.048544\n13  ElasticNet   50.0   0.256668  0.040044\n8        Lasso   50.0   0.262696  0.031598\n14  ElasticNet  100.0   0.262700  0.031594\n9        Lasso  100.0   0.284141  0.021419","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>alpha</th>\n      <th>rmse_mean</th>\n      <th>rmse_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>ElasticNet</td>\n      <td>0.1</td>\n      <td>0.179957</td>\n      <td>0.049057</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Lasso</td>\n      <td>0.1</td>\n      <td>0.189868</td>\n      <td>0.047422</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>ElasticNet</td>\n      <td>1.0</td>\n      <td>0.191479</td>\n      <td>0.050963</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ridge</td>\n      <td>50.0</td>\n      <td>0.194772</td>\n      <td>0.045467</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ridge</td>\n      <td>100.0</td>\n      <td>0.194774</td>\n      <td>0.045474</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Ridge</td>\n      <td>0.1</td>\n      <td>0.194775</td>\n      <td>0.045473</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ridge</td>\n      <td>10.0</td>\n      <td>0.194777</td>\n      <td>0.045473</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ridge</td>\n      <td>1.0</td>\n      <td>0.194778</td>\n      <td>0.045473</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Lasso</td>\n      <td>1.0</td>\n      <td>0.198569</td>\n      <td>0.052737</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>ElasticNet</td>\n      <td>10.0</td>\n      <td>0.233650</td>\n      <td>0.050588</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Lasso</td>\n      <td>10.0</td>\n      <td>0.238661</td>\n      <td>0.048544</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>ElasticNet</td>\n      <td>50.0</td>\n      <td>0.256668</td>\n      <td>0.040044</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Lasso</td>\n      <td>50.0</td>\n      <td>0.262696</td>\n      <td>0.031598</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>ElasticNet</td>\n      <td>100.0</td>\n      <td>0.262700</td>\n      <td>0.031594</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Lasso</td>\n      <td>100.0</td>\n      <td>0.284141</td>\n      <td>0.021419</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"import os, json\n\nOUT = \"outputs/models\"\nos.makedirs(OUT, exist_ok=True)\n\n# 1) comparison table\ndf_results.to_csv(os.path.join(OUT, \"week_8_regularization_cv_table.csv\"), index=False)\n\n# 2) best extract (RMSE min. standard)\nbest_overall = df_results.iloc[0].to_dict()\nbest_ridge = df_results[df_results[\"model\"]==\"Ridge\"].iloc[0].to_dict()\nbest_lasso = df_results[df_results[\"model\"]==\"Lasso\"].iloc[0].to_dict()\nbest_enet  = df_results[df_results[\"model\"]==\"ElasticNet\"].iloc[0].to_dict()\n\nweek8 = {\n    \"week\": \"Week 8\",\n    \"task\": \"Regularization & model selection\",\n    \"metric\": \"RMSE on log(SalePrice)\",\n    \"cv\": \"5-fold CV, shuffle=True, random_state=42\",\n    \"alphas_tested\": sorted([float(a) for a in df_results[\"alpha\"].unique().tolist()]),\n    \"best_overall\": best_overall,\n    \"best_by_model\": {\n        \"Ridge\": best_ridge,\n        \"Lasso\": best_lasso,\n        \"ElasticNet\": best_enet\n    },\n    \n    \"selected_model\": \"Ridge\",\n    \"selection_reason\": \"Selected for stability/robustness across alpha values in high-dimensional one-hot encoded feature space.\"\n}\n\nwith open(os.path.join(OUT, \"week_8_regularization_selection.json\"), \"w\") as f:\n    json.dump(week8, f, indent=2)\n\nprint(\"Saved:\",\n      \"week_8_regularization_cv_table.csv\",\n      \"week_8_regularization_selection.json\")\nprint(\"Models dir:\", sorted(os.listdir(OUT)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:53:49.907023Z","iopub.execute_input":"2026-02-05T00:53:49.907505Z","iopub.status.idle":"2026-02-05T00:53:49.929436Z","shell.execute_reply.started":"2026-02-05T00:53:49.907455Z","shell.execute_reply":"2026-02-05T00:53:49.927517Z"}},"outputs":[{"name":"stdout","text":"Saved: week_8_regularization_cv_table.csv week_8_regularization_selection.json\nModels dir: ['baseline_ridge_cv.json', 'fe_ridge_cv.json', 'week_8_regularization_cv_table.csv', 'week_8_regularization_selection.json', 'week_9_gbdt_feature_importance_top20.csv', 'week_9_model_family_comparison.csv', 'week_c_gbdt_feature_importance_top20.csv', 'week_c_model_family_comparison.csv']\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### Interpretation: Regularization and Model Selection\n\nRegularized linear models were compared under cross-validation to evaluate both\npredictive performance and stability. While Lasso and ElasticNet achieved slightly\nlower average error at specific regularization strengths, their performance was\nmore sensitive to hyperparameter choice.\n\nRidge regression demonstrated consistent performance across a wide range of alpha\nvalues, indicating greater robustness for this high-dimensional, one-hot encoded\nfeature space. Based on this tradeoff, Ridge was selected as the final linear model.\n","metadata":{}},{"cell_type":"markdown","source":"## Week 9 — Tree-based Models & Model Evaluation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\n# target\ny = np.log1p(train[\"SalePrice\"])\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\nrmse_scorer = make_scorer(rmse, greater_is_better=False)\n\n# feature split (Week A FE fixed)\nnum_cols = X_fe.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = [c for c in X_fe.columns if c not in num_cols]\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))]), num_cols),\n        (\"cat\", Pipeline([\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n        ]), cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:53:49.930754Z","iopub.execute_input":"2026-02-05T00:53:49.931163Z","iopub.status.idle":"2026-02-05T00:53:49.950474Z","shell.execute_reply.started":"2026-02-05T00:53:49.931129Z","shell.execute_reply":"2026-02-05T00:53:49.948579Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"models = [\n    (\"RandomForest\", RandomForestRegressor(\n        n_estimators=300,\n        max_depth=None,\n        random_state=42,\n        n_jobs=-1\n    )),\n    (\"GradientBoosting\", GradientBoostingRegressor(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42\n    ))\n]\n\nrows = []\n\nfor name, model in models:\n    pipe = Pipeline([\n        (\"preprocess\", preprocess),\n        (\"model\", model),\n    ])\n    scores = cross_val_score(pipe, X_fe, y, cv=cv, scoring=rmse_scorer)\n    rows.append({\n        \"model\": name,\n        \"rmse_mean\": float((-scores).mean()),\n        \"rmse_std\": float((-scores).std()),\n    })\n\ndf_tree = pd.DataFrame(rows).sort_values(\"rmse_mean\")\ndf_tree\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:53:49.952303Z","iopub.execute_input":"2026-02-05T00:53:49.952760Z","iopub.status.idle":"2026-02-05T00:55:16.280450Z","shell.execute_reply.started":"2026-02-05T00:53:49.952712Z","shell.execute_reply":"2026-02-05T00:55:16.278753Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"              model  rmse_mean  rmse_std\n1  GradientBoosting   0.131918  0.018892\n0      RandomForest   0.142959  0.018119","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>rmse_mean</th>\n      <th>rmse_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>GradientBoosting</td>\n      <td>0.131918</td>\n      <td>0.018892</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>RandomForest</td>\n      <td>0.142959</td>\n      <td>0.018119</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"import os, json\nimport pandas as pd\n\nOUT = \"outputs/models\"\nos.makedirs(OUT, exist_ok=True)\n\n# Read Week 7 baseline from the file\nwith open(os.path.join(OUT, \"baseline_ridge_cv.json\"), \"r\") as f:\n    base = json.load(f)\n\ncompare = pd.DataFrame([\n    {\"model_family\":\"Linear\", \"model\":\"Ridge (baseline)\", \"rmse_mean\":base[\"rmse_mean\"], \"rmse_std\":base[\"rmse_std\"]},\n])\n\ntree_block = df_tree.copy()\ntree_block[\"model_family\"] = \"Tree\"\n\ncompare = pd.concat(\n    [compare, tree_block[[\"model_family\",\"model\",\"rmse_mean\",\"rmse_std\"]]],\n    ignore_index=True\n)\n\ncompare.to_csv(os.path.join(OUT, \"week_9_model_family_comparison.csv\"), index=False)\n\nprint(\"Saved: week_9_model_family_comparison.csv\")\ncompare\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:55:16.282546Z","iopub.execute_input":"2026-02-05T00:55:16.283118Z","iopub.status.idle":"2026-02-05T00:55:16.309731Z","shell.execute_reply.started":"2026-02-05T00:55:16.283061Z","shell.execute_reply":"2026-02-05T00:55:16.308085Z"}},"outputs":[{"name":"stdout","text":"Saved: week_9_model_family_comparison.csv\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"  model_family             model  rmse_mean  rmse_std\n0       Linear  Ridge (baseline)   0.196570  0.045086\n1         Tree  GradientBoosting   0.131918  0.018892\n2         Tree      RandomForest   0.142959  0.018119","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model_family</th>\n      <th>model</th>\n      <th>rmse_mean</th>\n      <th>rmse_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Linear</td>\n      <td>Ridge (baseline)</td>\n      <td>0.196570</td>\n      <td>0.045086</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Tree</td>\n      <td>GradientBoosting</td>\n      <td>0.131918</td>\n      <td>0.018892</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Tree</td>\n      <td>RandomForest</td>\n      <td>0.142959</td>\n      <td>0.018119</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"### Interpretation: Linear vs Tree-based Models\n\nTree-based models significantly outperformed the selected linear baseline,\ndemonstrating their ability to capture nonlinear relationships and feature interactions\npresent in the housing data. In particular, Gradient Boosting achieved the lowest\ncross-validated error, indicating strong bias reduction compared to linear assumptions.\n\nThis comparison highlights the tradeoff between interpretability and predictive power,\nand motivates the use of tree-based models for final external evaluation.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nOUT = \"outputs/models\"\nos.makedirs(OUT, exist_ok=True)\n\n# 1) GBDT pipeline\ngbdt = GradientBoostingRegressor(\n    n_estimators=300,\n    learning_rate=0.05,\n    max_depth=3,\n    random_state=42\n)\n\ngbdt_pipe = Pipeline([\n    (\"preprocess\", preprocess),\n    (\"model\", gbdt),\n])\n\n# 2) Fit to full train\ngbdt_pipe.fit(X_fe, y)\n\n# 3) one-hot included feature names\npre = gbdt_pipe.named_steps[\"preprocess\"]\n\nnum_features = num_cols\ncat_ohe = pre.named_transformers_[\"cat\"].named_steps[\"onehot\"]\ncat_features = cat_ohe.get_feature_names_out(cat_cols).tolist()\n\nfeature_names = list(num_features) + list(cat_features)\n\n# 4) importance extract + organization\nimportances = gbdt_pipe.named_steps[\"model\"].feature_importances_\nfi = pd.DataFrame({\n    \"feature\": feature_names,\n    \"importance\": importances\n}).sort_values(\"importance\", ascending=False)\n\n# 5) top 20 save\ntop20 = fi.head(20).reset_index(drop=True)\ntop20.to_csv(os.path.join(OUT, \"week_9_gbdt_feature_importance_top20.csv\"), index=False)\n\ntop20\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:55:16.312359Z","iopub.execute_input":"2026-02-05T00:55:16.312907Z","iopub.status.idle":"2026-02-05T00:55:20.598847Z","shell.execute_reply.started":"2026-02-05T00:55:16.312842Z","shell.execute_reply":"2026-02-05T00:55:20.597033Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"             feature  importance\n0            TotalSF    0.357268\n1        OverallQual    0.354751\n2          TotalBath    0.054270\n3       YearRemodAdd    0.023059\n4         GarageCars    0.017405\n5        OverallCond    0.016085\n6            LotArea    0.013870\n7   GarageFinish_Unf    0.013323\n8         GarageArea    0.011650\n9         Fireplaces    0.011506\n10         GrLivArea    0.011478\n11      CentralAir_N    0.010451\n12          HouseAge    0.009432\n13          1stFlrSF    0.006733\n14        BsmtFinSF1    0.005598\n15         YearBuilt    0.005166\n16  MSZoning_C (all)    0.005041\n17    KitchenQual_TA    0.005007\n18      CentralAir_Y    0.004665\n19          2ndFlrSF    0.004338","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TotalSF</td>\n      <td>0.357268</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>OverallQual</td>\n      <td>0.354751</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TotalBath</td>\n      <td>0.054270</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>YearRemodAdd</td>\n      <td>0.023059</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GarageCars</td>\n      <td>0.017405</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>OverallCond</td>\n      <td>0.016085</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>LotArea</td>\n      <td>0.013870</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>GarageFinish_Unf</td>\n      <td>0.013323</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>GarageArea</td>\n      <td>0.011650</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Fireplaces</td>\n      <td>0.011506</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>GrLivArea</td>\n      <td>0.011478</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>CentralAir_N</td>\n      <td>0.010451</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>HouseAge</td>\n      <td>0.009432</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1stFlrSF</td>\n      <td>0.006733</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>BsmtFinSF1</td>\n      <td>0.005598</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>YearBuilt</td>\n      <td>0.005166</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>MSZoning_C (all)</td>\n      <td>0.005041</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>KitchenQual_TA</td>\n      <td>0.005007</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>CentralAir_Y</td>\n      <td>0.004665</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2ndFlrSF</td>\n      <td>0.004338</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"### Interpretation: Feature Importance and Model Behavior\n\nFeature importance analysis from the Gradient Boosting model highlights that overall\nhouse size and quality are the dominant drivers of price prediction. In particular,\nengineered features such as TotalSF and TotalBath rank among the most influential,\nconfirming that aggregating raw area and bathroom counts provides meaningful signal\nbeyond individual floor-level measurements.\n\nQuality-related attributes, including OverallQual and indicators of remodeling,\nalso contribute substantially, reflecting non-linear effects that are not easily\ncaptured by linear models. The presence of both raw features (e.g., GrLivArea,\nLotArea) and engineered features among the top-ranked variables suggests that the\ntree-based model effectively combines original and derived representations.\n\nOverall, this analysis supports the earlier performance comparison, demonstrating\nthat tree-based methods better capture complex interactions and nonlinearities in\nhousing data. These findings justify the use of Gradient Boosting for final external\nevaluation, while the linear baseline remains valuable for interpretability and\nmodel validation.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}